#!/usr/bin/env python3
"""
aquifer tool:  helping developers get build information for source files they want to test.
"""

import argparse
import os
import json
import logging
import sys
import subprocess
import re
import tempfile

TOOL_NAME = "aquifer"
TOOL_PATH = os.path.dirname(__file__)
COMPILE_CMD_NAME = "compile_commands.json"

PROJECT_NAME = os.path.basename(os.getcwd())
OUT_DB_NAME = "Makefile.aquifer"
OUT_JSON_NAME = "internal_rep.json"

MAKE_FILEUT = "FILEUT"
MAKE_FUNCUT = "FUNCUT"
MAKE_INCLUDE = "INC"
MAKE_DEFINE = "DEFINES"
MAKE_DEPENDENCIES = "DEPENDENCIES"

def build(args):
    """ Generate a json build database from given compile commands.
    If not compile commands path given, generate compile commands.
    No return"""

    comp_cmds_path = args.compile_commands_json_path
    root_dir = args.root_dir
    write_to_json = args.write_json
    save_path = args.save_path

    define_name = args.define_name
    if define_name:
        global MAKE_DEFINE
        MAKE_DEFINE = define_name

    include_name = args.include_name
    if include_name:
        global MAKE_INCLUDE
        MAKE_INCLUDE = include_name

    dependency_name = args.dependency_name
    if dependency_name:
        global MAKE_DEPENDENCIES
        MAKE_DEPENDENCIES = dependency_name

    dep_extension = args.dependency_extension

    # Load the generated compilation commands, if possible
    if not os.path.exists(comp_cmds_path):
        logging.error("Specified path does not point to an existing file: %s", comp_cmds_path)
        sys.exit(1)

    # get compilation commands
    out_db_path = OUT_DB_NAME
    internal_rep_path = OUT_JSON_NAME
    comp_cmds = None
    with open(comp_cmds_path, "r") as handle:
        comp_cmds = json.load(handle)

    # Extract relevant information
    internal_rep = {"files":{}}
    cflow_cmd = ["cflow"]

    for comp_cmd in comp_cmds:
        internal_file = {}
        # file name. TODO Only consider non-test files.
        file_path = str(comp_cmd['file'])
        # if file_path.endswith("test.c"):
        #     continue

        file_name = os.path.basename(file_path)
        internal_file["name"] = file_name
        # internal_file["name"] = file_name
        # file_dir = os.path.dirname(file_path)
        # internal_file["directory"] = file_dir
        if dep_extension:
            internal_file["alt_path"] = change_extension(file_path, dep_extension)
        else:
            internal_file["alt_path"] = None

        #COMMAND
        command_split = comp_cmd['command'].split()
        command_include_dirs = []
        command_defines = []
        for cmd_tup in enumerate(command_split):
            cmd_ind = cmd_tup[0]
            next_arg = "" if cmd_ind == len(command_split)-1 else command_split[cmd_ind+1]
            include = get_flag_arg("-I", command_split[cmd_ind], next_arg)
            if include:
                command_include_dirs.append(include)
            # include = get_flag_arg("-isystem", command_split[cmd_ind], next_arg)
            # if include:
            #     command_include_dirs.append(include)
            define = get_flag_arg("-D", command_split[cmd_ind], next_arg)
            if define:
                command_defines.append(define)

        # fill in intrernal representation
        internal_file[MAKE_INCLUDE] = command_include_dirs
        internal_file[MAKE_DEFINE] = command_defines
        internal_file["functions"] = {}

        # collect makefile information
        if file_name in internal_rep["files"]:
            path1 = file_path
            path2 = internal_rep["files"][file_name]["abspath"]
            logging.warning("Clashing file name: %s  <>  %s", path1, path2)
        else:
            internal_rep["files"][file_path] = internal_file

    # HANDLING DEPENDENCIES
    # get all header and source files in the project,
    # add to cflow command
    h_and_c_files = find_h_and_c(root_dir)
    cflow_cmd.extend(h_and_c_files)

    # run CFLOW, and output to file
    temp_file = tempfile.NamedTemporaryFile()
    tf_path = temp_file.name

    cflow_cmd.extend(["-A", "--no-main", "-o"+tf_path, "--brief"])
    subprocess.call(cflow_cmd)

    # parse CFLOW output
    parse_cflow(internal_rep, temp_file)

    # Write to files
    if write_to_json:
        write_internal_representation(internal_rep, write_to_json)
    else:
        write_internal_representation(internal_rep, internal_rep_path)

    # if save_path:
    #     write_makefile(internal_rep, save_path)
    # else:
    #     write_makefile(internal_rep, out_db_path)


def parse_cflow(rep, temp_file):
    """parse cflow output file. integrate it into json internal representation"""

    current_at_level = []
    for cur_line_b in temp_file:
        cur_line = cur_line_b.decode("utf-8") # TODO handle encoding in a more general manner?
        # get depth of current line
        leading_spaces = len(cur_line) - len(cur_line.lstrip(" "))
        cur_depth = leading_spaces // 4
        if leading_spaces % 4:
            logging.warning("Line has %d leading spaces: %s", leading_spaces, cur_line[:-1])

        #add function to internal rep
        regex = (r"(?P<fct>\w+)\(\)"
                 r"( <.+ at (?P<file>.+):\d+>"
                 r"( (?P<rec>\(R\)))?:)?"
                 r"( \[see (?P<ref>\d+)\])?")
        match = re.match(regex, cur_line.strip())
        if not match:
            logging.warning("Regex did not match for \"%s\"", cur_line[:-1])
            continue

        cf_func = match["fct"]
        cf_file = match["file"]
        cf_ref = match["ref"]
        if cf_file and not cf_ref:
            if cf_file in rep["files"]:
                if cf_func in rep["files"][cf_file]["functions"]:
                    logging.warning("duplicate entry for %s in %s", cf_func,
                                    cf_file)
                rep["files"][cf_file]["functions"][cf_func] = {}
            else:
                logging.warning("%s not found in internal representation. Added.", cf_file)
                rep["files"][cf_file] = {"functions" : {cf_func: {}}}

        #handle function callings
        cur_node = (cf_func, cf_file)

        if cur_depth < len(current_at_level) - 1:
            current_at_level = current_at_level[:cur_depth + 1]
            current_at_level[cur_depth] = cur_node
        elif cur_depth == len(current_at_level) - 1:
            current_at_level[cur_depth] = cur_node
        elif cur_depth == len(current_at_level):
            current_at_level.append(cur_node)
        elif cur_depth > len(current_at_level):
            logging.error("jump in depth at line NOT AVAILABLE")
            sys.exit(1)

        if cur_depth != 0:
            #add to 1 depth less
            parent_depth = cur_depth - 1
            parent_func = current_at_level[parent_depth][0]
            parent_file = current_at_level[parent_depth][1]
            if not parent_file in rep["files"]:
                logging.error("Parent file %s of calling function %s of "
                              "called function %s is not found in internal rep.",
                              parent_file, parent_func, cf_func)
                sys.exit(1)
            else:
                rep["files"][parent_file]["functions"]\
                    [parent_func][cf_func] = cf_file

    temp_file.close()

def find_h_and_c(root_dir):
    """From the project root, return a list of all header and source files in the project"""
    all_h_and_c = []
    for root, _, files in os.walk(root_dir):
        for file in files:
            if file.endswith(".h") or file.endswith(".c"):
                f_path = os.path.abspath(os.path.join(root, file))
                all_h_and_c.append(f_path)

    return all_h_and_c


def change_extension(path, new_ext):
    """change the extension of a file given it's path"""
    name, _ = os.path.splitext(path)
    return name + "." + new_ext


def write_internal_representation(rep, path):
    """ output a file continaing the json internal representation used in the tool """
    outjson = json.dumps(rep, indent=4)
    with open(path, "w") as handle:
        print(outjson, file=handle)
    print("-- Saved the internal representation at: {}".format(os.path.abspath(path)))


def write_makefile(rep, path):
    """ write the Makefile """
    all_files = rep["files"]
    out_lines = []
    for file in all_files:
        dep = file
        if all_files[file]["alt_path"]:
            dep = all_files[file]["alt_path"]
        out_lines.append("ifneq (,$(findstring %s, %s))" % (dep, make_value(MAKE_DEPENDENCIES)))

        #iterate through includes
        for inc in all_files[file][MAKE_INCLUDE]:
            out_lines.extend(add_if_not_there(inc, MAKE_INCLUDE))

        #iterate through defines
        for define in all_files[file][MAKE_DEFINE]:
            out_lines.extend(add_if_not_there(define, MAKE_DEFINE))

        out_lines.append("endif\n")

    # Write to output file
    with open(path, "w") as output:
        print("\n".join(out_lines), file=output)
    print("-- Created aquifer Makefile at {}".format(os.path.abspath(path)))


def add_if_not_there(file, tag):
    """ add conditional append of Make variable to list of test lines """
    lines = []
    lines.append("  ifeq (,$(findstring %s, %s))" % (file, make_value(tag)))
    lines.append("    %s" % add_to_var(tag, file))
    lines.append("  endif")
    return lines


def make_value(var):
    """ return MAKE syntax for accessing a variable value """
    return "$(" + str(var) + ")"


def add_to_var(var, term):
    """ return correct MAKE syntax for adding a value to a variable. """
    return "%s += %s" % (var, term)


def get_flag_arg(flag, arg, next_arg):
    """ if string corresponds to correct flag, return corresponding argument. """
    if len(arg) < len(flag):
        return None
    if len(arg) == len(flag) and arg == flag:
        return arg + " " + next_arg
    if len(arg) > len(flag) and arg[:len(flag)] == flag:
        return arg


def print_file(args):
    """ Print build information for a selected file. No return. """

    path_to_file = args.file_path
    print("print is not yet implemented - file {}".format(path_to_file))


def render(args):
    """ Generate a web interface containing build data for all fioles. No return. """

    print("render is not yet implemented")


def parse_args():
    """parse arguments"""
    parser = argparse.ArgumentParser(description=__doc__)
    subparser = parser.add_subparsers(help="Available commands for {}".format(TOOL_NAME))

    # build command
    parser_build = subparser.add_parser('build', help="Generate a build database \
        for the current project.")
    parser_build.add_argument("compile_commands_json_path",
                              help="path to compile_commands json file.")
    parser_build.add_argument("-r", "--root-dir", required=True,
                              help="root directory for the project under test")
    parser_build.add_argument("-sp", "--save-path",
                              help="file path where the output Makefile will be saved")
    parser_build.add_argument("-wj", "--write-json",
                              help="output a file containing a json internal \
                              representation of the stored information. \
                              Possible to add save path")
    parser_build.add_argument("-def", "--define_name",
                              help="label for defines used in the Makefile")
    parser_build.add_argument("-inc", "--include_name",
                              help="label for includes used in the Makefile")
    parser_build.add_argument("-dep", "--dependency_name",
                              help="label for dependencies used in the Makefile")
    parser_build.add_argument("-dx", "--dependency_extension",
                              help="alternate extension for dependecies")
    parser_build.set_defaults(func=build)

    # print command
    parser_print = subparser.add_parser('print', help="Print build information \
        for a selected file.")
    parser_print.add_argument("file_path", default="",
                              help="path to file to output info for.")
    parser_print.set_defaults(func=print_file)

    # render command
    parser_render = subparser.add_parser('render', help="Generate a web interface \
        containing {} data.".format(TOOL_NAME))
    parser_render.set_defaults(func=render)

    # parse and run
    return parser.parse_args()


def main():
    """ Main function """

    logging.basicConfig(filename=os.path.join(os.path.dirname(__file__), "aquifer.log"),
                        level=logging.DEBUG)

    arguments = parse_args()
    arguments.func(arguments)
    print()


if __name__ == "__main__":
    main()
