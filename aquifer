#!/usr/bin/env python3
"""
aquifer tool:  helping developers get build information for source files they want to test.
"""

import argparse
import os
import json
import logging
import sys
import subprocess
import re
import tempfile
from dataclasses import dataclass

TOOL_NAME = "aquifer"
TOOL_PATH = os.path.dirname(__file__)
COMPILE_CMD_NAME = "compile_commands.json"

PROJECT_NAME = os.path.basename(os.getcwd())
OUT_JSON_NAME = "internal_rep.json"
OUT_MF_NAME = "Makefile.aquifer"

JSON_FILE = "files"
JSON_NAME = "name"
JSON_INC = "includes"
JSON_DEF = "defines"
JSON_FCT = "functions"

MAKE_FILEUT = "FILEUT"
MAKE_FUNCUT = "FUNCUT"
MAKE_INCLUDE = "INC"
MAKE_DEFINE = "DEFINES"
MAKE_DEPENDENCIES = "DEPENDENCIES"

def build(args):
    """ Generate a json build database from given compile commands.
    If not compile commands path given, generate compile commands.
    No return"""

    comp_cmds_path = args.compile_commands
    root_dir = args.root_dir
    json_path = args.json_path
    # Load the generated compilation commands, if possible
    if not os.path.exists(comp_cmds_path):
        logging.error("Specified path does not point to an existing file: %s", comp_cmds_path)
        sys.exit(1)

    # get compilation commands
    internal_rep_path = OUT_JSON_NAME
    comp_cmds = None
    with open(comp_cmds_path, "r") as handle:
        comp_cmds = json.load(handle)

    # Extract relevant information
    internal_rep = {JSON_FILE:{}}
    cflow_cmd = ["cflow"]

    for comp_cmd in comp_cmds:
        internal_file = {}
        # file name. TODO Only consider non-test files.
        file_path = str(comp_cmd['file'])
        # if file_path.endswith("test.c"):
        #     continue

        file_name = os.path.basename(file_path)
        internal_file[JSON_NAME] = file_name

        # COMPILE COMMAND
        command_split = comp_cmd['command'].split()
        command_include_dirs = []
        command_defines = []
        for cmd_tup in enumerate(command_split):
            cmd_ind = cmd_tup[0]
            next_arg = "" if cmd_ind == len(command_split)-1 else command_split[cmd_ind+1]
            include = get_flag_arg("-I", command_split[cmd_ind], next_arg)
            if include:
                command_include_dirs.append(include)
            # include = get_flag_arg("-isystem", command_split[cmd_ind], next_arg)
            # if include:
            #     command_include_dirs.append(include)
            define = get_flag_arg("-D", command_split[cmd_ind], next_arg)
            if define:
                command_defines.append(define)

        # fill in internal representation
        internal_file[JSON_INC] = command_include_dirs
        internal_file[JSON_DEF] = command_defines
        internal_file[JSON_FCT] = {}

        # collect makefile information
        if file_path in internal_rep[JSON_FILE]:
            logging.warning("Clashing file name: %s", file_path)
        else:
            internal_rep[JSON_FILE][file_path] = internal_file

    # HANDLING DEPENDENCIES
    # get all header and source files in the project,
    # add to cflow command
    h_and_c_files = find_h_and_c(root_dir)
    cflow_cmd.extend(h_and_c_files)

    # run CFLOW, and output to file
    temp_file = tempfile.NamedTemporaryFile()
    tf_path = temp_file.name
    cflow_cmd.extend(["-A", "--no-main", "-o"+tf_path, "--brief"])
    subprocess.call(cflow_cmd)

    # parse CFLOW output
    parse_cflow(internal_rep, temp_file)

    # Write internal representation to files
    if json_path:
        write_internal_representation(internal_rep, json_path)
    else:
        write_internal_representation(internal_rep, internal_rep_path)


def parse_cflow(rep, temp_file):
    """parse cflow output file. integrate it into json internal representation"""

    current_at_level = []
    for cur_line_b in temp_file:
        cur_line = cur_line_b.decode("utf-8") # TODO handle encoding in a more general manner?
        # get depth of current line
        leading_spaces = len(cur_line) - len(cur_line.lstrip(" "))
        cur_depth = leading_spaces // 4
        if leading_spaces % 4:
            logging.warning("Line has %d leading spaces: %s", leading_spaces, cur_line[:-1])

        #add function to internal rep
        regex = (r"(?P<fct>\w+)\(\)"
                 r"( <.+ at (?P<file>.+):\d+>"
                 r"( (?P<rec>\(R\)))?:)?"
                 r"( \[see (?P<ref>\d+)\])?")
        match = re.match(regex, cur_line.strip())
        if not match:
            logging.warning("Regex did not match for \"%s\"", cur_line[:-1])
            continue

        cf_func = match["fct"]
        cf_file = match["file"]
        cf_ref = match["ref"]
        if cf_file and not cf_ref:
            if cf_file in rep[JSON_FILE]:
                if cf_func in rep[JSON_FILE][cf_file][JSON_FCT]:
                    logging.warning("duplicate entry for %s in %s", cf_func,
                                    cf_file)
                rep[JSON_FILE][cf_file][JSON_FCT][cf_func] = {}
            else:
                if cf_file.endswith(".c"):
                    logging.warning("source file <%s> not found"
                                    " in internal representation. Adding.", cf_file)
                elif cf_file.endswith(".h"):
                    logging.info("Header <%s> not found "
                                 "in internal representation. Adding.", cf_file)
                else:
                    logging.error("<%s> not found in internal representation. Adding.", cf_file)
                    sys.exit(1)
                add_to_int_rep(cf_file, cf_func, rep)

        #handle function callings
        cur_node = (cf_func, cf_file)

        if cur_depth < len(current_at_level) - 1:
            current_at_level = current_at_level[:cur_depth + 1]
            current_at_level[cur_depth] = cur_node
        elif cur_depth == len(current_at_level) - 1:
            current_at_level[cur_depth] = cur_node
        elif cur_depth == len(current_at_level):
            current_at_level.append(cur_node)
        elif cur_depth > len(current_at_level):
            logging.error("jump in depth at line NOT AVAILABLE")
            sys.exit(1)

        if cur_depth != 0:
            #add to 1 depth less
            parent_depth = cur_depth - 1
            parent_func = current_at_level[parent_depth][0]
            parent_file = current_at_level[parent_depth][1]
            if not parent_file in rep[JSON_FILE]:
                logging.error("Parent file %s of calling function %s of "
                              "called function %s is not found in internal rep.",
                              parent_file, parent_func, cf_func)
                sys.exit(1)
            else:
                rep[JSON_FILE][parent_file][JSON_FCT]\
                    [parent_func][cf_func] = cf_file

    temp_file.close()

def find_h_and_c(root_dir):
    """From the project root, return a list of all header and source files in the project"""
    all_h_and_c = []
    for root, _, files in os.walk(root_dir):
        for file in files:
            if file.endswith(".h") or file.endswith(".c"):
                f_path = os.path.abspath(os.path.join(root, file))
                all_h_and_c.append(f_path)
    return all_h_and_c


def add_to_int_rep(cf_file, cf_func, rep):
    """add a file to the internal json representation adequately"""
    f_info = {}
    f_info[JSON_NAME] = os.path.basename(cf_file)
    f_info[JSON_INC] = []
    f_info[JSON_DEF] = []
    f_info[JSON_FCT] = {cf_func:{}}
    rep[JSON_FILE][cf_file] = f_info


def write_internal_representation(rep, path):
    """ output a file continaing the json internal representation used in the tool """
    outjson = json.dumps(rep, indent=4)
    with open(path, "w") as handle:
        print(outjson, file=handle)
    print("-- Saved the internal representation at: {}".format(os.path.abspath(path)))


def get_flag_arg(flag, arg, next_arg):
    """ if string corresponds to correct flag, return corresponding argument. """
    if len(arg) < len(flag):
        return None
    if len(arg) == len(flag) and arg == flag:
        return arg + " " + next_arg
    if len(arg) > len(flag) and arg[:len(flag)] == flag:
        return arg


def create_makefile(args):
    """ Output a makefile containing relevant information for
    an input harness. No return. """

    harness_path = os.path.abspath(args.harness)
    save_path = os.path.join(os.path.dirname(harness_path), OUT_MF_NAME)
    if args.save_path:
        save_path = args.save_path

    json_path = OUT_JSON_NAME
    if args.json_path:
        json_path = args.json_path

    if args.define_name:
        global MAKE_DEFINE
        MAKE_DEFINE = args.define_name

    if args.include_name:
        global MAKE_INCLUDE
        MAKE_INCLUDE = args.include_name

    if args.dependency_name:
        global MAKE_DEPENDENCIES
        MAKE_DEPENDENCIES = args.dependency_name

    dep_extension = args.dependency_extension

    if not os.path.exists(harness_path) or not harness_path.endswith(".c"):
        logging.error("Specified path does not point to an existing .c file: %s", harness_path)
        sys.exit(1)

    if not os.path.exists(json_path):
        logging.error("Specified path does not point to an existing .json file: %s", json_path)
        sys.exit(1)

    int_rep = None
    with open(json_path, "r") as json_file:
        int_rep = json.load(json_file)

    makefile_list = build_makefile(int_rep, harness_path)

    # Write to output file
    with open(save_path, "w") as output:
        print("\n".join(makefile_list), file=output)
    print("-- Created aquifer Makefile at {}".format(os.path.abspath(save_path)))


def build_makefile(rep, h_path):
    """ build the makeffile """
    if h_path not in rep[JSON_FILE]:
        logging.error("<%s> not found in given internal representation. "
                      "Try rebuilding the internal rep.", h_path)
        sys.exit(1)

    out_lines = []
    out_lines.append("This file is generated autonatically by Aquifer")
    out_lines.append("")

    harness_info = MakefileInfo
    harness_entry = rep[JSON_FILE][h_path]
    all_files = rep[JSON_FILE]

    # for file in all_files:
    #     dep = file
    #     if all_files[file]["alt_path"]:
    #         dep = all_files[file]["alt_path"]
    #     out_lines.append("ifneq (,$(findstring %s, %s))" % (dep, make_value(MAKE_DEPENDENCIES)))

    #     #iterate through includes
    #     for inc in all_files[file][MAKE_INCLUDE]:
    #         out_lines.extend(add_if_not_there(inc, MAKE_INCLUDE))

    #     #iterate through defines
    #     for define in all_files[file][MAKE_DEFINE]:
    #         out_lines.extend(add_if_not_there(define, MAKE_DEFINE))

    #     out_lines.append("endif\n")
    return out_lines

@dataclass
class MakefileInfo:
    """ class that contains all the iunfo needed to create a makefile for a given harness """
    includes = []
    defines = []
    dependencies = []


def change_extension(path, new_ext):
    """change the extension of a file given it's path"""
    name, _ = os.path.splitext(path)
    return name + "." + new_ext


def add_if_not_there(file, tag):
    """ add conditional append of Make variable to list of test lines """
    lines = []
    lines.append("  ifeq (,$(findstring %s, %s))" % (file, make_value(tag)))
    lines.append("    %s" % add_to_var(tag, file))
    lines.append("  endif")
    return lines


def make_value(var):
    """ return MAKE syntax for accessing a variable value """
    return "$(" + str(var) + ")"


def add_to_var(var, term):
    """ return correct MAKE syntax for adding a value to a variable. """
    return "%s += %s" % (var, term)


def parse_args():
    """parse arguments"""
    parser = argparse.ArgumentParser(description=__doc__)
    subparser = parser.add_subparsers(help="Available commands for {}".format(TOOL_NAME))

    # build command
    parser_build = subparser.add_parser('build', help="Generate a build database \
        for the current project.")
    parser_build.add_argument("-cc", "--compile-commands", required=True,
                              help="path to compile_commands json file.")
    parser_build.add_argument("-r", "--root-dir", required=True,
                              help="root directory for the project under test")
    parser_build.add_argument("-jp", "--json_path",
                              help="output location for a file containing a json internal \
                              representation of the stored information.")
    parser_build.set_defaults(func=build)

    # print command
    parser_make = subparser.add_parser('makefile', help="Create a makefile \
        containing relevant info for a given harness.")
    parser_make.add_argument("-ha", "--harness", required=True,
                             help="path to harness for which we create a makefile.")
    parser_make.add_argument("-jp", "--json-path",
                             help="file path to JSON internal representation")
    parser_make.add_argument("-sp", "--save-path",
                             help="file path where the output Makefile will be saved")
    parser_make.add_argument("-def", "--define_name",
                             help="label for defines used in the Makefile")
    parser_make.add_argument("-inc", "--include_name",
                             help="label for includes used in the Makefile")
    parser_make.add_argument("-dep", "--dependency_name",
                             help="label for dependencies used in the Makefile")
    parser_make.add_argument("-dx", "--dependency_extension",
                             help="alternate extension for dependecies")
    parser_make.set_defaults(func=create_makefile)

    # parse and run
    return parser.parse_args()


def main():
    """ Main function """

    logging.basicConfig(filename=os.path.join(os.path.dirname(__file__), "aquifer.log"),
                        level=logging.DEBUG)

    arguments = parse_args()
    arguments.func(arguments)
    print()


if __name__ == "__main__":
    main()
