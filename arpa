#!/usr/bin/env python3
#   Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
#   Licensed under the Apache License, Version 2.0 (the "License").
#   You may not use this file except in compliance with the License.
#   You may obtain a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#   See the License for the specific language governing permissions and
#   limitations under the License.

"""
arpa tool:  helping developers get build information for source files they want to test.
"""

import argparse
import os
import json
import logging
import sys
import subprocess
import re
import tempfile
from dataclasses import dataclass
import voluptuous
import voluptuous.humanize
#TODO add the import in method to allow running without validation

LOG_FILE_NAME = "arpa.log"
TOOL_NAME = "arpa"
TOOL_PATH = os.path.dirname(__file__)
COMPILE_CMD_NAME = "compile_commands.json"

PROJECT_NAME = os.path.basename(os.getcwd())
OUT_JSON_NAME = "internal_rep.json"
OUT_MF_NAME = "Makefile.arpa"

JSON_INFO = "root"
JSON_FILE = "files"
JSON_NAME = "name"
JSON_INC = "includes"
JSON_DEF = "defines"
JSON_FCT = "functions"

CC_INCLUDE = "-I"
CC_DEFINE = "-D"

class CompileCommands:

    def __init__(self, cc_path):
        self.path = cc_path
        self.__validate()
        self.file_2_command = {}

    def __validate(self):
        if not os.path.exists(self.path):
            logging.error("Specified path does not point to an existing file: %s", self.path)
            sys.exit(1)

    def create_file_2_command_map(self):
        with open(self.path, "r") as handle:
            all_commands = json.load(handle)

        for compilation_command in all_commands:
            cc_file = str(compilation_command['file'])
            cc_command = compilation_command['command'].split()
            self.file_2_command[cc_file] = cc_command

    def __get_flag_arg(self, flag, arg, next_arg):
        """ if string corresponds to correct flag, return corresponding argument. """
        if arg.startswith(flag):
            if len(arg) == len(flag):
                return next_arg
            if len(arg) > len(flag):
                return arg[len(flag):]
        else:
            return None

    def __get_abspath(self, include):
        """transform included path to an absolute path """
        if os.path.isabs(include) and os.path.exists(include):
            return include
        path = os.path.join(os.path.dirname(self.path), include)
        if os.path.exists(path):
            return os.path.abspath(path)

        logging.error("include path not found at %s", path)
        sys.exit(1)

    def get_flags(self, prefix, file):
        command_split = self.file_2_command[file]
        all_flags = []
        for command_tuple in enumerate(command_split):
            cmd_ind = command_tuple[0]
            next_arg = "" if cmd_ind == len(command_split)-1 else command_split[cmd_ind+1]
            flag = self.__get_flag_arg(prefix, command_split[cmd_ind], next_arg)
            if flag:
                all_flags.append(flag)
        
        return all_flags

    def get_includes(self, file):
        all_includes = self.get_flags(CC_INCLUDE, file)
        all_includes_absolute = []
        for include_path in all_includes:
            all_includes_absolute.append(self.__get_abspath(include_path))
        return all_includes_absolute

    def get_defines(self, file):
        return self.get_flags(CC_DEFINE, file)

    # TODO handle other flags from the compilation commands, put them in CFLAGS

class InternalRepresentation:
    
    def __init__(self, root_path):
        self.representation = {JSON_INFO: root_path,
                               JSON_FILE: {}, }

    def __add_element(self, file_path, key, value):
        self.representation[JSON_FILE][file_path][key] = value

    def add_file_entry(self, file_path):
        file_name = os.path.basename(file_path)
        if file_path in self.representation[JSON_FILE]:
            logging.warning("Clashing file name: %s", file_path)
        else:
            self.representation[JSON_FILE][file_path] = {}
            self.__add_element(file_path, JSON_NAME, file_name)

    def add_defines(self, file_path, defines_list):
        self.__add_element(file_path, JSON_DEF, defines_list)

    def add_includes(self, file_path, includes_list):
        self.__add_element(file_path, JSON_INC, includes_list)

    def add_dependencies(self, file_2_dependencies):
        for file in file_2_dependencies:
            if file in self.representation[JSON_FILE]:
                self.representation[JSON_FILE][file][JSON_FCT] = file_2_dependencies[file]
            else:
                if file.endswith(".c"):
                    logging.info("source file <%s> not found"
                                " in internal representation. Adding.", file)
                elif file.endswith(".h"):
                    logging.debug("Header <%s> not found "
                                "in internal representation. Adding.", file)
                else:
                    logging.error("<%s> not found in internal representation. Exiting.", file)
                    sys.exit(1)
                self.representation[JSON_FILE][file] = {
                    JSON_NAME: os.path.basename(file),
                    JSON_INC: [],
                    JSON_DEF: [],
                    JSON_FCT: file_2_dependencies[file],
                }

        # TODO possibly remove?
        for file in self.representation[JSON_FILE]:
            if file not in file_2_dependencies:
                self.representation[JSON_FILE][file][JSON_FCT] = {}


    def validate(self):
        """ validate the internal representation using voluptuous """
        def h_c_file(value):
            if isinstance(value, str) and (value.endswith(".c") or value.endswith(".h")):
                return value
            raise voluptuous.Invalid("Not an existing .c or .h file.")

        def existing_file(value):
            if isinstance(value, str) and os.path.isfile(value):
                return value
            raise voluptuous.Invalid("Not an existing file.")

        def existing_directory(value):
            if isinstance(value, str) and os.path.isdir(value):
                return value
            raise voluptuous.Invalid("Not an existing directory.")

        def compiler_define(value):
            regex = re.compile(r"\w+(=.+)?")
            if isinstance(value, str) and regex.match(value):
                return value
            raise voluptuous.Invalid("Not a valid compiler define.")

        def function_name(value):
            regex = re.compile(r"\w+")
            if isinstance(value, str) and regex.match(value):
                return value
            raise voluptuous.Invalid("Not a valid function name.")

        schema = voluptuous.Schema({
            JSON_INFO: existing_directory,
            JSON_FILE: {
                voluptuous.All(h_c_file, existing_file): {
                    JSON_NAME: h_c_file,
                    JSON_INC: [existing_directory],
                    JSON_DEF: [compiler_define],
                    JSON_FCT: {
                        function_name: {
                            function_name: voluptuous.Any(voluptuous.All(h_c_file, existing_file), None)
                        }
                    }
                }
            }
        }, required=True)
        voluptuous.humanize.validate_with_humanized_errors(self.representation, schema)

    def write_to_file(self, from_run, path):
        """ output a file continaing the json internal representation used in the tool """
        # TODO improve this from_run thing
        outjson = json.dumps(self.representation, indent=4)
        with open(path, "w") as handle:
            print(outjson, file=handle)
        if not from_run:
            print("-- Saved the internal representation at: {}".format(os.path.abspath(path)))

class CflowInstance:

    def __init__(self):
        self.command = ["cflow"]
        self.file_2_dependencies = {}
        temp_file = tempfile.NamedTemporaryFile(delete=False)
        self.output_path = temp_file.name
        temp_file.close()


    def __add_to_command(self, item):
        self.command.append(item)


    def __find_h_and_c(self, root_dir):
        """From the project root, return a list of all header and source files in the project"""
        all_h_and_c = []
        exclude = ["aws-proof-build-assistant"]
        for root, dirs, files in os.walk(root_dir):
            dirs[:] = [d for d in dirs if d not in exclude]
            for file in files:
                if file.endswith(".h") or file.endswith(".c"):
                    f_path = os.path.abspath(os.path.join(root, file))
                    all_h_and_c.append(f_path)
        return all_h_and_c

    def prepare_command(self, root_path):
        h_and_c_files = self.__find_h_and_c(root_path)
        self.command.extend(h_and_c_files)
        self.command.extend(["-A", "--no-main", "-o%s" %(self.output_path), "--brief"])

    def run_command(self):
        proc = subprocess.Popen(self.command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True)
        _, errors = proc.communicate()
        for cflow_message in errors.split("\n"):
            logging.warning("cflow stderr > %s", cflow_message)
    
    def parse_output(self):
        """parse cflow output file. integrate it into json internal representation"""

        current_at_level = []
        with open(self.output_path, "r") as cflow_out:
            cnt = 0
            for cur_line in cflow_out:
                # get depth of current line
                leading_spaces = len(cur_line) - len(cur_line.lstrip(" "))
                cur_depth = leading_spaces // 4
                if leading_spaces % 4:
                    logging.warning("Line has %d leading spaces: %s", leading_spaces, cur_line[:-1])

                #add function to internal rep
                regex = (r"(?P<fct>\w+)\(\)"
                        r"( <.+ at (?P<file>.+):\d+>"
                        r"( (?P<rec>\(R\)))?:)?"
                        r"( \[see (?P<ref>\d+)\])?")
                match = re.match(regex, cur_line.strip())
                if not match:
                    logging.warning("Regex did not match for \"%s\"", cur_line[:-1])
                    continue

                cf_func = match["fct"]
                cf_file = match["file"]
                cf_ref = match["ref"]
                if cf_file and not cf_ref:
                    if cf_file in self.file_2_dependencies:
                        if cf_func in self.file_2_dependencies[cf_file]:
                            logging.warning("duplicate entry for %s in %s", cf_func,
                                            cf_file)                    
                        self.file_2_dependencies[cf_file][cf_func] = {}
                    else:
                        self.file_2_dependencies[cf_file] = {cf_func:{}}

                #handle function callings
                cur_node = (cf_func, cf_file)

                if cur_depth < len(current_at_level) - 1:
                    current_at_level = current_at_level[:cur_depth + 1]
                    current_at_level[cur_depth] = cur_node
                elif cur_depth == len(current_at_level) - 1:
                    current_at_level[cur_depth] = cur_node
                elif cur_depth == len(current_at_level):
                    current_at_level.append(cur_node)
                elif cur_depth > len(current_at_level):
                    logging.error("jump in depth at line {}".format(cnt))
                    sys.exit(1)

                if cur_depth != 0:
                    #add to 1 depth less
                    parent_depth = cur_depth - 1
                    parent_func = current_at_level[parent_depth][0]
                    parent_file = current_at_level[parent_depth][1]

                    if parent_file in self.file_2_dependencies:
                        self.file_2_dependencies[parent_file][parent_func][cf_func] = cf_file
                    else:
                        logging.error("Parent file %s of calling function %s of "
                                    "called function %s is not found in cflow output.",
                                    parent_file, parent_func, cf_func)
                        sys.exit(1)
                cnt += 1


@dataclass
class MakefileContents:
    """ class that contains all the info needed to create a makefile for a given harness """
    includes = set()
    defines = set()
    dependencies = set()
    missing_dependencies = {}
    func_calls = {} # not currently used, but this may be useful going forward

@dataclass
class MakefileOrderedContents:
    """ class that contains all the info needed to create a makefile for a given harness """
    includes = []
    defines = []
    proof_sources = []
    project_sources = []
    other_dependencies = []
    missing_dependencies = {}

class Makefile:

    def __init__(self, proof_directory):
        self.contents = MakefileContents()
        self.contents_ordered = MakefileOrderedContents()

        self.save_path = ""
        self.directory = proof_directory
        self.textual_representation = []

    def set_default_save_path(self, dir_path):
        self.save_path = os.path.join(dir_path, OUT_MF_NAME)
    
    def set_save_path(self, path):
        self.save_path = path

    def __make_val(self, var):
        """ return MAKE syntax for accessing a variable value """
        return "$(" + str(var) + ")"

    def __change_extension(self, path, ext):
        """change the extension of a file given it's path"""
        name, _ = os.path.splitext(path)
        return "%s.%s" % (name, ext)

    def __add_initial_comment_to_text(self):
        self.textual_representation.append("# This file is generated automatically by Arpa")
        self.textual_representation.append("")

    def __add_directory_paths_to_text(self, args):
        self.textual_representation.append("%s = %s" % (args.make_root_name, args.make_root_path))
        self.textual_representation.append("%s = %s" % (args.make_helper_name, args.make_helper_path))
        self.textual_representation.append("%s = %s" % (args.make_proofs_name, args.make_proofs_path))
        self.textual_representation.append("")

    def __add_entry_info_to_text(self, args):
        entry_name, ext = os.path.splitext(os.path.basename(self.directory.harness_path))
        if not entry_name.endswith("_harness"):
            logging.error("invalid harness name: %s", self.directory.harness_path)
            sys.exit(1)
        function_name = entry_name[:-len("_harness")]
        if args.shorten_entry:
            entry_name = function_name
        self.textual_representation.append("%s = %s" % (args.entry_name, entry_name))
        self.textual_representation.append("%s = %s.c" % (args.entry_file, self.__make_val(args.entry_name)))
        self.textual_representation.append("")

    def __add_arpa_makefile_to_text(self, args):
        self.textual_representation.append("include %s" %(args.makefile_common_path))

    def __find_custom_info_recursively(self, args, internal_rep, current_path, relevant_functions):
        # info = self.contents
        """ recursively look for interesting information related to Makefile generation """
        # Recursion issues are supposed to be handled by cflow
        current_json_entry = internal_rep[JSON_FILE][current_path]
        self.contents.includes.update(set(current_json_entry[JSON_INC]))
        self.contents.defines.update(set(current_json_entry[JSON_DEF]))

        file_2_called_functions = {}
        for fct in relevant_functions:
            for called_fct, called_file in current_json_entry[JSON_FCT][fct].items():
                if called_file:
                    # case where a file location is specified for the called function
                    called_file_w_ext = called_file
                    if args.dependency_extension:
                        called_file_w_ext = self.__change_extension(called_file, args.dependency_extension)
                    self.contents.dependencies.add(called_file_w_ext)

                    # add called function to the list of recursive future calls
                    if called_file in file_2_called_functions:
                        file_2_called_functions[called_file].add(called_fct)
                    else:
                        file_2_called_functions[called_file] = {called_fct}

                    # map of which function calls which other function
                    if called_file in self.contents.func_calls:
                        self.contents.func_calls[called_file_w_ext].append(called_fct)
                    else:
                        self.contents.func_calls[called_file_w_ext] = [called_fct]
                else:
                    #case where the location of a called fct is not known
                    entry = (current_path, fct)
                    if entry in self.contents.missing_dependencies:
                        self.contents.missing_dependencies[entry].add(called_fct)
                    else:
                        self.contents.missing_dependencies[entry] = {called_fct}

        for file_path, file_functions in file_2_called_functions.items():
            self.__find_custom_info_recursively(args, internal_rep, file_path, file_functions)
    
    def __order_custom_info(self, args):
        """ add Makefile shortcuts to paths in MakefileInfo """
        includes_2_shortcut_path = self.__shortcut_path(args, self.contents.includes, CC_INCLUDE)
        [self.contents_ordered.includes.extend(includes_2_shortcut_path[k]) for k in includes_2_shortcut_path]

        self.contents_ordered.defines = self.__add_prefix(self.contents.defines, CC_DEFINE)

        dependencies_2_shortcut_path = self.__shortcut_path(args, self.contents.dependencies, "")
        self.contents_ordered.proof_sources = dependencies_2_shortcut_path["proof"]
        self.contents_ordered.project_sources = dependencies_2_shortcut_path["project"]
        self.contents_ordered.other_dependencies = dependencies_2_shortcut_path["none"]
        self.contents_ordered.missing_dependencies = self.contents.missing_dependencies # unordered

    def __shortcut_path(self, args, paths, prefix):
        #TODO this is very complicated
        """ replace project root and proofs root path with Makefile variable """
        new_paths = {"proof":[],
                    "project":[],
                    "none":[]}
        for path in paths:
            key, new_path = self.__check_com_prefixes(args, path)

            new_paths[key].append("%s%s" % (prefix, new_path))

        for key in new_paths:
            new_paths[key] = sorted(new_paths[key])
        return new_paths

    def __check_com_prefixes(self, args, path):
        #TODO this is very complicated
        """ check if prefix is a prefix of path. return updated path """
        cat2prefix_path2name = {"proof" : {
            args.make_proofs_path: args.make_proofs_name,
            args.make_helper_path: args.make_helper_name,
            args.make_stubs_path: args.make_stubs_name},
                            "project" : {
                                args.make_root_path: args.make_root_name}}
        for cat in cat2prefix_path2name:
            for prefix_path in cat2prefix_path2name[cat]:
                if os.path.commonprefix([path, prefix_path]) == prefix_path:
                    if path == prefix_path:
                        return (cat, self.__make_val(cat2prefix_path2name[cat][prefix_path]))

                    rel_path = os.path.relpath(path, prefix_path)
                    return (cat, os.path.join(self.__make_val(cat2prefix_path2name[cat][prefix_path]), rel_path))

        return ("none", path)

    def __add_prefix(self, items, prefix):
        """ add prefix flag to input items. Return list """
        new_items = ["%s%s" % (prefix, i) for i in items]
        return sorted(new_items)
    
    
    def __add_to_makefile(self, elements, label):
        """ return a list of Makefile commands associtaed to a specific variable """
        lines = [self.__add_to_var(label, e) for e in elements]
        lines.append("")
        return lines


    def __add_missing_to_makefile(self, mapping):
        """ return a list of comments for the missing dependencies"""
        all_lines = []
        for source, fcts in mapping.items():
            file = source[0]
            func = source[1]
            all_lines.extend(["# * <%s>   in %s:%s" % (f, file, func) for f in fcts])
        
        return all_lines

    def __add_to_var(self, var, term):
        """ return correct MAKE syntax for adding a value to a variable. """
        return "%s += %s" % (var, term)
    
    def __add_custom_info_to_text(self, args):
        lines_to_add = []
        lines_to_add.extend(self.__add_to_makefile(self.contents_ordered.defines, args.define_name))
        lines_to_add.extend(self.__add_to_makefile(self.contents_ordered.includes, args.include_name))
        lines_to_add.extend(self.__add_to_makefile(self.contents_ordered.proof_sources, args.make_proofs_sources))
        lines_to_add.extend(self.__add_to_makefile(self.contents_ordered.project_sources, args.make_root_sources))
        lines_to_add.extend(self.__add_to_makefile(self.contents_ordered.other_dependencies, args.dependency_name))

        #add missing dependencies section
        lines_to_add.append("")
        lines_to_add.append("# The proof also calls into the following functions, whose source")
        lines_to_add.append("# files could not be determined.")
        lines_to_add.append("# You may need to find the files these functions reside in")
        lines_to_add.append("# and add them to the $(PROJECT_SOURCES) array.")
        lines_to_add.extend(self.__add_missing_to_makefile(self.contents_ordered.missing_dependencies))

        self.textual_representation.extend(lines_to_add)


    def build(self, args, internal_rep):
        harness_path = self.directory.harness_path
        if harness_path not in internal_rep[JSON_FILE]:
            logging.error("<%s> not found in given internal representation. "
                        "Try rebuilding the internal rep.", harness_path)
            sys.exit(1)

        self.__add_initial_comment_to_text()
        # self.__add_directory_paths_to_text(args)
        # self.__add_entry_info_to_text(args)

        harness_functions = internal_rep[JSON_FILE][harness_path][JSON_FCT].keys()
        self.__find_custom_info_recursively(args, internal_rep, harness_path, harness_functions)
        self.__order_custom_info(args)
        self.__add_custom_info_to_text(args)


        # self.__add_arpa_makefile_to_text(args)


    def save(self):
        with open(self.save_path, "w") as output:
            print("\n".join(self.textual_representation), file=output)
        print("-- Created arpa Makefile at {}".format(os.path.abspath(self.save_path)))


class ProofDirectory:

    def __init__(self):
        self.path = ""
        self.harness_path = ""

    def set_harness_path(self, input_path):
        """ find harness file in current directory. Return its path """
        harness_path = ""
        # if harness path is given
        if input_path:
            harness_path = os.path.abspath(input_path)
            if not os.path.exists(harness_path):
                logging.error("Specified path does not point to an existing file: %s", harness_path)
                sys.exit(1)
            self.harness_path = harness_path
            self.path = os.path.dirname(harness_path)
            return

        # look for harness in cwd
        found_harness = False
        for file in os.listdir("."):
            if file.endswith("_harness.c"):
                if found_harness:
                    logging.error("too many harness files in current directory!!")
                    sys.exit(1)
                harness_path = os.path.abspath(file)
                found_harness = True
        
        if not found_harness:
            logging.error("no harness found in current directory!!")
            sys.exit(1)
        self.harness_path = harness_path
        self.path = os.path.dirname(harness_path)


class ExistingInternalRepresentation(InternalRepresentation):

    def __init__(self, json_path):
        # print("in")
        # super(ExistingInternalRepresentation, self).__init__()
        self.json_path = json_path
        self.__validate_path()
        self.__load_json()

    def __validate_path(self):
        if not os.path.exists(self.json_path):
            logging.error("Specified path does not point to an existing .json file: %s", self.json_path)
            sys.exit(1)

    def __load_json(self):
        with open(self.json_path, "r") as json_file:
            self.representation = json.load(json_file)




def build(args):
    """ this function is called only when the arpa build command is called """
    builder(args, False)

def builder(args, from_run):
    """ Generate a json build database from given compile commands.
    If not compile commands path given, generate compile commands.
    No return"""

    cc = CompileCommands(args.compile_commands)
    internal_repr = InternalRepresentation(os.path.abspath(args.root_dir))
    cflow_instance = CflowInstance()

    cc.create_file_2_command_map()
    for file in cc.file_2_command:
        internal_repr.add_file_entry(file)
        internal_repr.add_includes(file, cc.get_includes(file))
        internal_repr.add_defines(file, cc.get_defines(file))

    cflow_instance.prepare_command(args.root_dir)
    cflow_instance.run_command()
    cflow_instance.parse_output()

    internal_repr.add_dependencies(cflow_instance.file_2_dependencies)
    internal_repr.validate()

    if args.json_path:
        internal_repr.write_to_file(from_run, args.json_path)
    else:
        internal_repr.write_to_file(from_run, OUT_JSON_NAME)

def makefile(args):
    """ Output a makefile containing relevant information for
    an input harness. No return. """

    proof_directory = ProofDirectory()
    makefile = Makefile(proof_directory)
    input_rep = ExistingInternalRepresentation(args.json_path)

    proof_directory.set_harness_path(args.harness)
    # TODO make below line part of ProofDirectory class
    args.make_proofs_path = os.path.abspath(proof_directory.path)

    if args.save_path:
        makefile.set_save_path(args.save_path)
    else:
        makefile.set_default_save_path(proof_directory.path)

    handle_args(args, input_rep.representation[JSON_INFO])

    makefile.build(args, input_rep.representation)
    makefile.save()
    
    # TODO remove exclude-function-name and unwindset-name flags


def handle_args(args, root_path):
    """ handle comand line flags that modify global variables"""

    # store root path
    if args.make_root_path:
        args.make_root_path = os.path.abspath(os.path.join(os.getcwd(), args.make_source_path))
    else:
        args.make_root_path = root_path

    # handle proofs path
    args.make_helper_path = handle_path(args, args.make_helper_path)
    args.make_stubs_path = handle_path(args, args.make_stubs_path)


def handle_path(args, path_ut):
    joined_path_ut = os.path.join(args.make_root_path, path_ut)
    if path_ut:
        if os.path.isabs(path_ut) and os.path.exists(path_ut):
            return path_ut
        elif os.path.exists(joined_path_ut):
            return os.path.abspath(joined_path_ut)
        elif os.path.exists(path_ut):
            return os.path.abspath(path_ut)
    
    logging.error("Specified proofs directory is incorrect: %s", path_ut)
    sys.exit(1)


def run(args):
    """ build internal representation and create a Makefil for a given harness """

    # tempfile for storing json int_rep
    temp_file = tempfile.NamedTemporaryFile()
    args.json_path = temp_file.name

    builder(args, True)
    makefile(args)

    temp_file.close()


def parse_args():
    """parse arguments"""
    # potentialTODO get inputs from JSON
    parser = argparse.ArgumentParser(description=__doc__)
    subparser = parser.add_subparsers(help="Available commands for {}".format(TOOL_NAME))

    # build command
    # default = called from project root
    parser_run_build = argparse.ArgumentParser(add_help=False)
    parser_run_build.add_argument("-cc", "--compile-commands", required=True,
                                  metavar="F",
                                  help="path to compile_commands json file.")

    parser_build = subparser.add_parser('build', parents=[parser_run_build],
                                        help="Generate a JSON build database \
        for the current project. Assumed cwd = roject root.")
    parser_build.add_argument("-r", "--root-dir", default=os.getcwd(),
                              metavar="DIR",
                              help="root directory for the project under test")
    parser_build.add_argument("-jp", "--json_path", default=OUT_JSON_NAME,
                              metavar="F",
                              help="output location for a file containing a json internal \
        representation of the stored information.")
    parser_build.set_defaults(func=build)

    # makefile command
    # default = called from harness directory
    parser_run_make = argparse.ArgumentParser(add_help=False)
    parser_run_make.add_argument("-ha", "--harness", metavar="F",
                                 help="path to harness for which we create a makefile.")
    parser_run_make.add_argument("-sp", "--save-path", metavar="F",
                                 help="file path where the output Makefile will be saved")

    # parser_run_make.add_argument("-p", "--project", choices=['s2n', 'freertos'],
    #                              help="predefine configurartion for a given project")
    parser_run_make.add_argument("-ef", "--entry-file", default="HARNESS_FILE",
                                 metavar="V",
                                 help="label for entry file used in the Makefile")
    parser_run_make.add_argument("-en", "--entry-name", default="HARNESS_ENTRY",
                                 metavar="V",
                                 help="label for entry type used in the Makefile")
    parser_run_make.add_argument("-sh", "--shorten-entry", action="store_true",
                                 help="shorten harness name when writing entry")
    parser_run_make.add_argument("-def", "--define-name", default="DEFINES",
                                 metavar="V",
                                 help="label for defines used in the Makefile")
    parser_run_make.add_argument("-inc", "--include-name", default="INCLUDES",
                                 metavar="V",
                                 help="label for includes used in the Makefile")
    parser_run_make.add_argument("-dep", "--dependency-name", default="DEPENDENCIES",
                                 metavar="V",
                                 help="label for dependencies used in the Makefile")
    parser_run_make.add_argument("-xfn", "--exclude-function-name", default="REMOVE_FUNCTION_BODY",
                                 metavar="V",
                                 help="label for function body exclusion used in the Makefile")
    parser_run_make.add_argument("-unw", "--unwindset-name", default="UNWINDSET",
                                 metavar="V",
                                 help="label for dependencies used in the Makefile")
    parser_run_make.add_argument("-dx", "--dependency-extension",
                                 metavar="EXT",
                                 help="alternate extension for dependecies")
    # parser_run_make.add_argument("-mc", "--makefile-common-path",
    #                              default="../Makefile.common",
    #                              metavar="DIR",
    #                              help="path to Makefile.common")

    parser_run_make.add_argument("-mrn", "--make-root-name", default="SRCDIR",
                                 metavar="V",
                                 help="Makefile variable name for root directory")
    parser_run_make.add_argument("-mrs", "--make-root-sources", default="PROJECT_SOURCES",
                                 metavar="V",
                                 help="Makefile variable name for source file under test")
    parser_run_make.add_argument("-mrp", "--make-root-path",
                                 metavar="DIR",
                                 help="path makefile real root directory")

    parser_run_make.add_argument("-mhn", "--make-helper-name", default="PROOF_SOURCE",
                                 metavar="V",
                                 help="Makefile variable name for proofs directory")
    parser_run_make.add_argument("-mhp", "--make-helper-path", default="tests/cbmc/sources",
                                 metavar="DIR",
                                 help="path makefile proofs directory")

    parser_run_make.add_argument("-msn", "--make-stubs-name", default="PROOF_STUB",
                                 metavar="V",
                                 help="Makefile variable name for proof stubs directory")
    parser_run_make.add_argument("-msp", "--make-stubs-path", default="tests/cbmc/stubs",
                                 metavar="DIR",
                                 help="path makefile proof stubs directory")

    parser_run_make.add_argument("-mpn", "--make-proofs-name", default="PROOFDIR",
                                 metavar="V",
                                 help="Makefile variable name for proofs directory")
    parser_run_make.add_argument("-mps", "--make-proofs-sources", default="PROOF_SOURCES",
                                 metavar="V",
                                 help="Makefile variable name for proofs source file")

    parser_make = subparser.add_parser('makefile', parents=[parser_run_make],
                                       help="Create a makefile \
        from the internal representation for a given harness. Assumed cwd = harness directory.")
    parser_make.add_argument("-jp", "--json-path", required=True,
                             metavar="F",
                             help="file path to JSON internal representation")
    parser_make.set_defaults(func=makefile)

    # run command
    # default = called from harness directory
    parser_run = subparser.add_parser('run', parents=[parser_run_build, parser_run_make],
                                      help="Generate a Makefile from scratch \
        for a given harness. Assumed cwd = harness directory.")
    parser_run.add_argument("-r", "--root-dir", required=True, metavar="DIR",
                            help="root directory for the project under test")
    parser_run.set_defaults(func=run)

    # parse
    return parser.parse_args()


def main():
    """ Main function """
    arguments = parse_args()
    # log_file = os.path.join(arguments.root_dir, LOG_FILE_NAME)
    # Below is to make debugging simpler
    log_file = os.path.join(TOOL_PATH, "arpa.log")
    if os.path.isfile(log_file) :
        os.remove(log_file)
    logging.basicConfig(filename=log_file,
                        level=logging.DEBUG)

    arguments.func(arguments)


if __name__ == "__main__":
    main()
